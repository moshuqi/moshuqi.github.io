---
layout: post
title:  通过Python爬虫抓取漫画图片
category: "Python"
---

无聊浏览某漫画网站（你懂的。-_-），每次翻页时都需要重新请求整个页面，页面杂七杂八的内容过多，导致页面加载过程耗时略长。于是决定先把图片先全部保存到本地。本文的主要内容，就是讲解如何通过一个爬虫程序，自动将所有图片抓取到本地。

先来看网页大概张什么样。：）

漫画封面列表界面：

![image1]({{ site.url }}/assets/Cartoon/1.jpg)

每部漫画点击进去的界面

![image2]({{ site.url }}/assets/Cartoon/2.jpg)

漫画列表每一页有几十部漫画，网站暂时有15页列表，每部漫画页数有10+页到近200页不等，所以所有漫画包含的图片总数还是比较可观的，通过手动将每张图右键另存基本不可能完成。接下来将一步一步展示，如何用Python实现一个简单爬虫的程序，将网页上所有漫画全保存到本地。Python用的是2.7版本。

<h4>首先，何为网络爬虫？</h4>

网络爬虫（又被称为网页蜘蛛，网络机器人），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。


**一般而言，简单的爬虫实现主要分为两步：**

*	获取指定网页html源码；
*	通过正则表达式提取出源码中的目标内容。	

<h3>本文程序实现大体步骤：</h3>
1.	获取每页漫画列表的url；
2.	获取每页漫画列表中每一部漫画的url；
3.	获取每部漫画每一页图片的url；
4.	通过图片的url将图片保存到本地。

（总之就是通过for循环遍历的过程。）

<h3>获取每页漫画列表的url</h3>

程序最开始处理的页面为：[http://www.xeall.com/shenshi](http://www.xeall.com/shenshi/)，即漫画列表的首页。绅士，你懂的。。

先创建一个文件**Gentleman.py**，引入需要用到的库：

	#coding:utf-8
	
	import urllib2
	import re
	import os
	import zlib

定义一个类，和初始化函数：

	class Gentleman:
	    def __init__(self, url, path):
	        exists = os.path.exists(path)
	        if not exists:
	            print "文件路径无效."
	            exit(0)
	
	        self.base_url = url
	        self.path = path


***url***为漫画列表首页url

***path***为图片保存在本地的路径，做个错误检测，判断本地是否存在对应的路径

接下来定义 ***get_content*** 方法，获取指定url的html源码内容：

	def get_content(self, url):
        # 打开网页
        try:
            request = urllib2.Request(url)
            response = urllib2.urlopen(request, timeout=20)

            # 将网页内容解压缩
            decompressed_data = zlib.decompress(response.read(), 16 + zlib.MAX_WBITS)

            # 网页编码格式为 gb2312
            content = decompressed_data.decode('gb2312', 'ignore')
            # print content
            return content
        except Exception, e:
            print e
            print "打开网页: " + url + "失败."
            return None

***urllib2.Request()*** 通过指定的url构建一个请求，***urllib2.urlopen()*** 获取请求返回的结果，**timeout**设为20秒无响应则做超时处理。***urllib2.urlopen()*** 有可能出现打开网页错误的情况，所以做了异常处理，保证在返回**40X**、**50X**之类的时候程序不会退出。后续相关的操作也都会做异常的处理。

原网页为压缩过的，请求到之后必须先进行解压缩处理。

通过 **charset=gb2312** 可知解码所需格式。

判断网页是否为压缩的编码类型，可以通过打印语句：

	response.info().get('Content-Encoding')

打印结果为：

	'gzip'

函数的返回结果为页面html源码文本。

跑起来试试：

	url = "http://www.xeall.com/shenshi"
	save_path = "/Users/moshuqi/Desktop/cartoon"
	
	gentleman = Gentleman(url, save_path)
	content = gentleman.get_content(url)
	
	print content
	
	
可以看到打印结果：

![image3]({{ site.url }}/assets/Cartoon/3.jpg)


接下来分析源码文件，看如何从中获取到每一页漫画列表的url。

页面上有个选择页的控件：

![image]({{ site.url }}/assets/Cartoon/4.jpg)

通过分析对应控件的源码，可知具体每一页所对应的url：

![image]({{ site.url }}/assets/Cartoon/5.jpg)

控件的 ***name*** 为 **sldd** ，通过搜索全文发现只有这一处“sldd”字段，所以该字段可用来做标识。

***option value*** 的值即为对应页的url。

定义 ***get_page_url_arr*** 方法，获取每一页的url，返回一个数组：

	def get_page_url_arr(self, content):
        pattern = re.compile('name=\'sldd\'.*?>(.*?)</select>', re.S)
        result = re.search(pattern, content)
        option_list = result.groups(1)

        pattern = re.compile('value=\'(.*?)\'.*?</option>', re.S)
        items = re.findall(pattern, option_list[0])

        arr = []
        for item in items:
            page_url = self.base_url + '/' + item
            arr.append(page_url)

        print "total pages: " + str(len(arr))
        return arr

未完待续。- -



